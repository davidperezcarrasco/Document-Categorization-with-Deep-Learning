{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "XObIjZluab-A",
        "cwnwy_t0ab-C"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWg8yk3HhVjs",
        "outputId": "4e34d394-0384-4f5d-8ecf-8f9a325d0980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change the path to where your notebook is located\n",
        "%cd \"gdrive/MyDrive/Introduction to Natural Language Processing\""
      ],
      "metadata": {
        "id": "SoelMM59hW-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171fa64a-dcc9-4814-e2d1-18003a8dc96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1syPiNVQUgh60gmvbP12NyL0c-k21dKeU/Introduction to Natural Language Processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XObIjZluab-A"
      },
      "source": [
        "## Collection of texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_Ah6R1Kab-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f5a3cd-f07e-4516-da36-7dcbc54b80cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 10788\n",
            "Size of a training set: 7769\n",
            "Size of a test set: 3019\n",
            "Number of various categories: 90\n",
            "Possible document categories:\n",
            "[('test/14843', ['acq']), ('test/14852', ['acq', 'copper'])]\n",
            "Example of a tokenized text:\n",
            "['SUMITOMO', 'BANK', 'AIMS', 'AT', 'QUICK', 'RECOVERY', ...]\n",
            "Corresponding raw text:\n",
            "SUMITOMO BANK AIMS AT QUICK RECOVERY FROM MERGER\n",
            "  Sumitomo Bank Ltd &lt;SUMI.T> is certain to\n",
            "  lose its status as Japan's most profitable bank as a result of\n",
            "  its merger with the Heiwa Sogo Bank, financial analysts said.\n",
            "      Osaka-based Sumitomo, with desposits of around 23.9\n",
            "  trillion yen, merged with Heiwa Sogo, a small, struggling bank\n",
            "  with an estimated 1.29 billion dlrs in unrecoverable loans, in\n",
            "  October.\n",
            "      But despite the link-up, Sumitomo President Koh Komatsu\n",
            "  told Reuters he is confident his bank can quickly regain its\n",
            "  position.\n",
            "      \"We'll be back in position in first place within three\n",
            "  years,\" Komatsu said in an interview.\n",
            "      He said that while the merger will initially reduce\n",
            "  Sumitomo's profitability and efficiency, it will vastly expand\n",
            "  Sumitomo's branch network in the Tokyo metropolitan area where\n",
            "  it has been relatively weak.\n",
            "      But financial analysts are divided on whether and how\n",
            "  quickly the gamble will pay off.\n",
            "      Some said Sumitomo may have paid too much for Heiwa Sogo in\n",
            "  view of the smaller bank's large debts. Others argue the merger\n",
            "  was more cost effective than creating a comparable branch\n",
            "  network from scratch.\n",
            "      The analysts agreed the bank was aggressive. It has\n",
            "  expanded overseas, entered the lucrative securities business\n",
            "  and geared up for domestic competition, but they questioned the\n",
            "  wisdom of some of those moves.\n",
            "      \"They've made bold moves to put everything in place. Now\n",
            "  it's largely out of their hands,\" said Kleinwort Benson Ltd\n",
            "  financial analyst Simon Smithson.\n",
            "      Among Sumitomo's problems are limits placed on its move to\n",
            "  enter U.S. Securities business by taking a share in American\n",
            "  investment bank Goldman, Sachs and Co.\n",
            "      Sumitomo last August agreed to pay 500 mln dlrs for a 12.5\n",
            "  pct limited partnership in the bank, but for the time being at\n",
            "  least, the Federal Reserve Board has forbidden them to exchange\n",
            "  personnel, or increase the business they do with each other.\n",
            "      \"The tie-up is widely looked on as a lame duck because the\n",
            "  Fed was stricter than Sumitomo expected,\" said one analyst.\n",
            "      But Komatsu said the move will pay off in time.\n",
            "      \"U.S. Regulations will change in the near future and if so,\n",
            "  we can do various things. We only have to wait two or three\n",
            "  years, not until the 21st century,\" Komatsu said.\n",
            "      Komatsu is also willing to be patient about possible routes\n",
            "  into the securities business at home.\n",
            "      Article 65 of the Securities and Exchange Act, Japan's\n",
            "  version of the U.S. Glass-Steagall Act, separates commercial\n",
            "  from investment banking.\n",
            "      But the walls between the two are crumbling and Komatsu\n",
            "  said he hopes further deregulation will create new\n",
            "  opportunities.\n",
            "      \"We need to find new business chances,\" Komatsu said. \"In some\n",
            "  cases these will be securities related, in some cases trust\n",
            "  bank related. That's the kind of deregulation we want.\"\n",
            "      Until such changes occur, Sumitomo will focus on such\n",
            "  domestic securities business as profitable government bond\n",
            "  dealing and strengthening relations with Meiko Securities Co\n",
            "  Ltd, in which it holds a five pct share, Komatsu said.\n",
            "      He said Sumitomo is cautiously optimistic about entering\n",
            "  the securities business here through its Swiss universal bank\n",
            "  subsidiary, Banca del Gottardo.\n",
            "       The Finance Ministry is expected to grant licences to\n",
            "  securities subsidiaries of U.S. Commercial banks soon,\n",
            "  following a similar decision for subsidiaries of European\n",
            "  universal banks in which the parent holds a less than 50 pct.\n",
            "      But Komatsu is reluctant to push hard for a similar\n",
            "  decision on a Gottardo subsidiary.\n",
            "      \"We don't want to make waves. We expect this will be allowed\n",
            "  in two or three years,\" he said.\n",
            "      Like other city banks, Sumitomo is also pushing to expand\n",
            "  lending to individuals and small and medium businesses to\n",
            "  replace disappearing demand from big business, he added.\n",
            "      The analysts said Sumitomo will have to devote a lot of\n",
            "  time to digesting its most recent initiatives, including the\n",
            "  merger with ailing Heiwa Sogo.\n",
            "      \"It's (Sumitomo) been bold in its strategies,\" said\n",
            "  Kleinwort's Smithson.\n",
            "      \"After that, it's a question of absorbing and juggling\n",
            "  around. It will be the next decade before we see if the\n",
            "  strategy is right or wrong.\"\n",
            "  \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download the collection of news with categories assigned\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "# Check what is inside the collection\n",
        "# You will see that each article might have more than 1 category\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "def collection_stats():\n",
        "    # List of documents\n",
        "    documents = reuters.fileids()\n",
        "    print(\"Number of documents: %d\" % (len(documents)))\n",
        "\n",
        "    train_docs = list(filter(lambda doc: doc.startswith(\"train\"),\n",
        "                        documents))\n",
        "    print(\"Size of a training set: %d\" % (len(train_docs)))\n",
        "\n",
        "    test_docs = list(filter(lambda doc: doc.startswith(\"test\"),\n",
        "                       documents))\n",
        "    print(\"Size of a test set: %d\" % (len(test_docs)))\n",
        "\n",
        "    # List of categories\n",
        "    categories = reuters.categories()\n",
        "    print(\"Number of various categories: %d\" % (len(categories)))\n",
        "\n",
        "    # Documents in a category\n",
        "    category_docs = reuters.fileids(\"acq\")\n",
        "\n",
        "    # Categories of a document\n",
        "    document_id_list = category_docs[:2]\n",
        "    print(\"Possible document categories:\")\n",
        "    print(list(map(lambda doc: tuple([doc, reuters.categories(doc)]), document_id_list)))\n",
        "\n",
        "    # Words for a document\n",
        "    document_id = document_id_list[0]\n",
        "    document_words = reuters.words(document_id)\n",
        "    print(\"Example of a tokenized text:\")\n",
        "    print(document_words)\n",
        "\n",
        "    # Raw text of a document\n",
        "    print(\"Corresponding raw text:\")\n",
        "    print(reuters.raw(document_id));\n",
        "collection_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vH7Kmszlab-G"
      },
      "outputs": [],
      "source": [
        "# Evaluate F-measure values for different classes\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def print_f1(model, x_test, y_test):\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    # Here we take the top class, but in fact, you can use more than one most-probable predictions\n",
        "    #   from probability distribution (e.g., when one example may correspond to several classes)\n",
        "    y_pred = [list(i).index(max(i)) for i in y_pred]\n",
        "    y_true = [list(i).index(max(i)) for i in y_test]\n",
        "    print(\"Macro F1-score: %.2f\" % (f1_score(y_true, y_pred, average='macro')))\n",
        "    print(\"Micro F1-score: %.2f\" % (f1_score(y_true, y_pred, average='micro')))\n",
        "    print(list(zip(reuters.get_label_names(), f1_score(y_true, y_pred, average=None))))\n",
        "    return f1_score(y_true, y_pred, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwnwy_t0ab-C"
      },
      "source": [
        "## First model (simple multi-layer perceptron)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import reuters\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 10000\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
        "                                                         test_split=0.2)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "print(num_classes, 'classes')\n",
        "\n",
        "print('Vectorizing sequence data...')\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train_mlp = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test_mlp = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('x_train shape:', x_train_mlp.shape)\n",
        "print('x_test shape:', x_test_mlp.shape)\n",
        "\n",
        "print('Convert class vector to binary class matrix '\n",
        "      '(for use with categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "print('Building model...')\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_mlp, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "score = model.evaluate(x_test_mlp, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "f1 = print_f1(model, x_test_mlp, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRILyP3Xseg0",
        "outputId": "9e5cc849-a2ec-4f55-e2cf-e6e3701b3bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "8982 train sequences\n",
            "2246 test sequences\n",
            "46 classes\n",
            "Vectorizing sequence data...\n",
            "x_train shape: (8982, 10000)\n",
            "x_test shape: (2246, 10000)\n",
            "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
            "y_train shape: (8982, 46)\n",
            "y_test shape: (2246, 46)\n",
            "Building model...\n",
            "Epoch 1/10\n",
            "127/127 [==============================] - 2s 8ms/step - loss: 1.6692 - accuracy: 0.6289 - val_loss: 1.1851 - val_accuracy: 0.7430\n",
            "Epoch 2/10\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.9077 - accuracy: 0.7920 - val_loss: 1.0146 - val_accuracy: 0.7786\n",
            "Epoch 3/10\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.5764 - accuracy: 0.8635 - val_loss: 0.9448 - val_accuracy: 0.8098\n",
            "Epoch 4/10\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3890 - accuracy: 0.9021 - val_loss: 0.9514 - val_accuracy: 0.8020\n",
            "Epoch 5/10\n",
            "127/127 [==============================] - 1s 9ms/step - loss: 0.3083 - accuracy: 0.9280 - val_loss: 0.9497 - val_accuracy: 0.8098\n",
            "Epoch 6/10\n",
            "127/127 [==============================] - 1s 10ms/step - loss: 0.2636 - accuracy: 0.9388 - val_loss: 0.9963 - val_accuracy: 0.8165\n",
            "Epoch 7/10\n",
            "127/127 [==============================] - 1s 11ms/step - loss: 0.2361 - accuracy: 0.9421 - val_loss: 0.9896 - val_accuracy: 0.8198\n",
            "Epoch 8/10\n",
            "127/127 [==============================] - 1s 10ms/step - loss: 0.1907 - accuracy: 0.9501 - val_loss: 1.0600 - val_accuracy: 0.8087\n",
            "Epoch 9/10\n",
            "127/127 [==============================] - 1s 10ms/step - loss: 0.1875 - accuracy: 0.9563 - val_loss: 1.0626 - val_accuracy: 0.8220\n",
            "Epoch 10/10\n",
            "127/127 [==============================] - 1s 10ms/step - loss: 0.1822 - accuracy: 0.9560 - val_loss: 1.0785 - val_accuracy: 0.8142\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 1.0818 - accuracy: 0.8108\n",
            "Test score: 1.0818123817443848\n",
            "Test accuracy: 0.8107746839523315\n",
            "71/71 [==============================] - 0s 4ms/step\n",
            "Macro F1-score: 0.62\n",
            "Micro F1-score: 0.81\n",
            "[('cocoa', 0.761904761904762), ('grain', 0.74235807860262), ('veg-oil', 0.6666666666666665), ('earn', 0.9182926829268293), ('acq', 0.8556806550665302), ('wheat', 0.0), ('copper', 0.896551724137931), ('housing', 0.5), ('money-supply', 0.6987951807228916), ('coffee', 0.7916666666666667), ('sugar', 0.9180327868852459), ('trade', 0.7039106145251396), ('reserves', 0.3529411764705882), ('ship', 0.6486486486486487), ('cotton', 0.6666666666666666), ('carcass', 0.30769230769230765), ('crude', 0.7475728155339805), ('nat-gas', 0.5882352941176471), ('cpi', 0.7428571428571429), ('money-fx', 0.6917293233082706), ('interest', 0.6043165467625898), ('gnp', 0.6666666666666666), ('meal-feed', 0.0), ('alum', 0.6923076923076924), ('oilseed', 0.5161290322580646), ('gold', 0.7368421052631579), ('tin', 0.7999999999999999), ('strategic-metal', 0.4), ('livestock', 0.6), ('retail', 0.8), ('ipi', 0.7368421052631579), ('iron-steel', 0.6363636363636364), ('rubber', 0.8421052631578948), ('heat', 0.8000000000000002), ('jobs', 0.4444444444444445), ('lei', 0.6666666666666666), ('bop', 0.631578947368421), ('zinc', 0.0), ('orange', 0.6666666666666666), ('pet-chem', 0.0), ('dlr', 0.4615384615384615), ('gas', 0.5), ('silver', 0.4), ('wpi', 0.8571428571428571), ('hog', 0.888888888888889), ('lead', 1.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxz853wKab-H"
      },
      "source": [
        "## Best Model (Bidirectional LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import reuters\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, GRU, SimpleRNN, Bidirectional\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from pyfasttext import FastText\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout, Bidirectional, TimeDistributed\n",
        "from keras.layers import LSTM, GRU, SimpleRNN\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Constants\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "batch_size = 32\n",
        "\n",
        "embedding_dim_glove = 100\n",
        "embedding_dim_fasttext = 300\n",
        "using_pyfasttext = False\n",
        "\n",
        "# Load the dataset and preprocess\n",
        "print('Loading data...')\n",
        "max_words = 30000\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2)\n",
        "word_index = reuters.get_word_index()\n",
        "\n",
        "# Ensure that classes are in a one-hot encoded format\n",
        "num_classes = max(max(y_train), max(y_test)) + 1\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train_seq = pad_sequences(x_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "x_test_seq = pad_sequences(x_test, maxlen=max_sequence_length, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "f2VYoxNPOc-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_f1_scores(model, x_test, y_test):\n",
        "    y_pred_prob = model.predict(x_test)\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
        "    print(f\"Macro F1-score: {macro_f1:.2f}\")\n",
        "    print(f\"Micro F1-score: {micro_f1:.2f}\")\n",
        "    return macro_f1\n",
        "\n",
        "def load_glove_embeddings(filepath):\n",
        "    embeddings_index = {}\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index, len(coefs)\n",
        "\n",
        "def load_fasttext_embeddings(filepath):\n",
        "    fasttext_model = FastText()\n",
        "    fasttext_model.load_model(filepath)\n",
        "    return fasttext_model\n",
        "\n",
        "def create_embedding_matrix_glove(word_index, embeddings_index, embedding_dim, max_words):\n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < max_words:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "def create_embedding_matrix_fasttext(word_index, fasttext_model, embedding_dim, max_words):\n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < max_words:\n",
        "            embedding_vector = fasttext_model.get_numpy_vector(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "def create_embedding_matrix(embedding_type, embedding_dim, word_index, max_words):\n",
        "    if embedding_type == 'glove':\n",
        "        filepath = f'glove.6B.{embedding_dim}d.txt'\n",
        "        embeddings_index, _ = load_glove_embeddings(filepath)\n",
        "        return create_embedding_matrix_glove(word_index, embeddings_index, embedding_dim, max_words)\n",
        "    elif embedding_type == 'fasttext':\n",
        "        fasttext_model = load_fasttext_embeddings('cc.en.300.bin')\n",
        "        return create_embedding_matrix_fasttext(word_index, fasttext_model, embedding_dim, max_words)"
      ],
      "metadata": {
        "id": "AnJ031ehOeOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_configurable_model(layer_type, num_units, dropout_rate, return_sequences, second_layer, embedding_dim, max_sequence_length, num_classes, optimizer, embedding_type, padding='post', truncating='post', stack_layers=2, trainable_embeddings=True):\n",
        "    if embedding_type == 'glove':\n",
        "        embeddings_index, _ = load_glove_embeddings('glove.6B.300d.txt' if embedding_dim == 300 else 'glove.6B.100d.txt')\n",
        "        embedding_matrix = create_embedding_matrix_glove(word_index, embeddings_index, embedding_dim, max_words)\n",
        "    else:\n",
        "        embedding_matrix = np.random.random((max_words, embedding_dim))\n",
        "\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=len(embedding_matrix),\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_sequence_length,\n",
        "        trainable=trainable_embeddings\n",
        "    )\n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    layer_out = embedded_sequences\n",
        "    for i in range(stack_layers - 1):\n",
        "        if layer_type == 'LSTM':\n",
        "            layer_out = LSTM(num_units, return_sequences=True)(layer_out)\n",
        "        elif layer_type == 'Bidirectional LSTM':\n",
        "            layer_out = Bidirectional(LSTM(num_units, return_sequences=True))(layer_out)\n",
        "\n",
        "    if layer_type == 'LSTM':\n",
        "        layer_out = LSTM(num_units, return_sequences=False)(layer_out)\n",
        "    elif layer_type == 'Bidirectional LSTM':\n",
        "        layer_out = Bidirectional(LSTM(num_units, return_sequences=False))(layer_out)\n",
        "\n",
        "    if dropout_rate > 0:\n",
        "        layer_out = Dropout(dropout_rate)(layer_out)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(layer_out)\n",
        "    model = Model(sequence_input, output)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "phbqhBToOkGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = [\n",
        "\n",
        "    {'layer_type': 'Bidirectional LSTM', 'num_units': 256, 'dropout_rate': 0.0, 'return_sequences': True, 'second_layer': True, 'embedding_dim': 300, 'max_sequence_length': 50, 'batch_size': 32, 'epochs': 10, 'optimizer': 'adam', 'padding': 'pre', 'truncating': 'pre', 'embedding_type': 'glove', 'stack_layers': 0}\n",
        "]"
      ],
      "metadata": {
        "id": "9iBPL0wHOr4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate each model\n",
        "results = []\n",
        "for config in configurations:\n",
        "    model = build_configurable_model(\n",
        "        layer_type=config['layer_type'],\n",
        "        num_units=config['num_units'],\n",
        "        dropout_rate=config['dropout_rate'],\n",
        "        return_sequences=config['return_sequences'],\n",
        "        second_layer=config['second_layer'],\n",
        "        embedding_dim=config['embedding_dim'],\n",
        "        max_sequence_length=config['max_sequence_length'],\n",
        "        num_classes=num_classes,\n",
        "        optimizer=config['optimizer'],\n",
        "        embedding_type=config['embedding_type'],\n",
        "        padding=config['padding'],\n",
        "        truncating=config['truncating'],\n",
        "        stack_layers=config.get('stack_layers', 2)\n",
        "    )\n",
        "\n",
        "    # Update input sequences with the specified padding and truncating methods\n",
        "    x_train_seq = pad_sequences(x_train, maxlen=config['max_sequence_length'], padding=config['padding'], truncating=config['truncating'])\n",
        "    x_test_seq = pad_sequences(x_test, maxlen=config['max_sequence_length'], padding=config['padding'], truncating=config['truncating'])\n",
        "\n",
        "    print(f\"Training {config['layer_type']} model with {config['num_units']} units, {config['embedding_dim']}d embeddings, {config['max_sequence_length']} sequence length, {config['batch_size']} batch size, and {config['epochs']} epochs using {config['optimizer']}...\")\n",
        "    model.fit(x_train_seq, y_train, epochs=config['epochs'], batch_size=config['batch_size'], validation_split=0.1, verbose=2)\n",
        "    scores = model.evaluate(x_test_seq, y_test, verbose=0)\n",
        "    print(f\"Test accuracy for {config['layer_type']} model: {scores[1] * 100:.2f}%\")\n",
        "    f1_macro = print_f1_scores(model, x_test_seq, y_test)\n",
        "    results.append((config['layer_type'], scores[1], f1_macro))\n",
        "\n",
        "# Sort results by accuracy\n",
        "results_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nResults ordered by accuracy:\")\n",
        "print(\"{:<20} | {:<10} | {:<10}\".format(\"Model Type\", \"Accuracy\", \"F1 Score\"))\n",
        "print(\"-\" * 40)\n",
        "for name, accuracy, f1_macro in results_sorted:\n",
        "    print(\"{:<20} | {:<10.2f} | {:<10.2f}\".format(name, accuracy * 100, f1_macro * 100))"
      ],
      "metadata": {
        "id": "DukykU_QPAlU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}